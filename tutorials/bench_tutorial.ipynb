{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RM-Gallery Bench 用户使用教程\n",
    "\n",
    "本教程将介绍如何使用 RM-Gallery 的 EvaluationRunner 进行模型输出的批量评测。我们将涵盖以下内容：\n",
    "1. EvaluationRunner 的基本概念\n",
    "2. 如何配置和使用 EvaluationRunner\n",
    "3. 如何准备评测数据\n",
    "4. 如何运行评测并分析结果\n",
    "\n",
    "## 什么是 EvaluationRunner？\n",
    "\n",
    "EvaluationRunner 是 RM-Gallery 中用于批量运行评估任务的组件。它允许用户并行地对多个数据样本进行评估，支持多种评估器（Grader）同时运行，并提供了并发控制功能。\n",
    "\n",
    "让我们首先导入必要的模块："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading environment variables...\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from typing import List\n",
    "\n",
    "from rm_gallery.core.data import DataSample, validate_data_samples\n",
    "from rm_gallery.core.grader import GraderMode, LLMGrader, FunctionGrader, GraderScore\n",
    "from rm_gallery.core.model.template import Template, RequiredField\n",
    "from rm_gallery.core.runner.evaluation import EvaluationRunner\n",
    "from dotenv import dotenv_values\n",
    "dotenv_values()\n",
    "print(\"Loading environment variables...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 创建评估器（Graders）\n",
    "\n",
    "在使用 EvaluationRunner 之前，我们需要定义一个或多个评估器。评估器是实际执行评估逻辑的组件。\n",
    "\n",
    "### 1.1 创建基于函数的评估器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "基于函数的评估器创建完成！\n"
     ]
    }
   ],
   "source": [
    "# 使用装饰器创建一个简单的基于函数的评估器\n",
    "@FunctionGrader.wrap\n",
    "async def length_grader(answer: str, **kwargs) -> GraderScore:\n",
    "    \"\"\"根据回答长度评分的评估器 - 更长的回答得分更高\"\"\"\n",
    "    # 简单地根据字符数评分，归一化到0-1之间\n",
    "    score = min(len(answer) / 100.0, 1.0)  # 假设100个字符为满分\n",
    "    return GraderScore(\n",
    "        score=score,\n",
    "        reason=f\"回答长度为 {len(answer)} 个字符\"\n",
    "    )\n",
    "\n",
    "# 创建另一个基于函数的评估器\n",
    "@FunctionGrader.wrap\n",
    "async def keyword_grader(answer: str, query: str, **kwargs) -> GraderScore:\n",
    "    \"\"\"检查回答中是否包含查询关键词的评估器\"\"\"\n",
    "    # 提取查询中的关键词（简化处理）\n",
    "    keywords = query.lower().split()\n",
    "    answer_lower = answer.lower()\n",
    "    \n",
    "    # 计算包含的关键词比例\n",
    "    matched_keywords = sum(1 for keyword in keywords if keyword in answer_lower)\n",
    "    score = matched_keywords / len(keywords) if keywords else 1.0\n",
    "    \n",
    "    return GraderScore(\n",
    "        score=score,\n",
    "        reason=f\"查询中的 {len(keywords)} 个关键词中有 {matched_keywords} 个在回答中找到\"\n",
    "    )\n",
    "\n",
    "print(\"基于函数的评估器创建完成！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 创建基于LLM的评估器\n",
    "\n",
    "对于更复杂的评估任务，我们可以使用基于LLM的评估器："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "基于LLM的评估器创建完成！\n"
     ]
    }
   ],
   "source": [
    "# 定义一个基于LLM的评估器模板\n",
    "quality_template = {\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": (\n",
    "                \"你是一个专业的评估助手，负责评估AI模型回答的质量。请根据以下标准评估回答：\\n\"\n",
    "                \"1. 相关性：回答是否与问题相关\\n\"\n",
    "                \"2. 准确性：回答是否准确无误\\n\"\n",
    "                \"3. 完整性：回答是否完整地解答了问题\\n\"\n",
    "                \"请给出0到1之间的分数，1表示完美回答，0表示完全不相关或错误的回答。\"\n",
    "            )\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": (\n",
    "                \"问题：{query}\\n\"\n",
    "                \"回答：{answer}\\n\\n\"\n",
    "                \"请根据上述标准评估这个回答的质量，并以以下JSON格式输出：\\n\"\n",
    "                \"{{\\n  \\\"score\\\": <0到1之间的分数>,\\n  \\\"reason\\\": \\\"评估理由\\\"\\n}}\"\n",
    "            )\n",
    "        }\n",
    "    ],\n",
    "    \"required_fields\": [\n",
    "        RequiredField(\n",
    "            name=\"query\",\n",
    "            type=\"string\",\n",
    "            position=\"data\",\n",
    "            description=\"用户提出的问题\"\n",
    "        ),\n",
    "        RequiredField(\n",
    "            name=\"answer\",\n",
    "            type=\"string\",\n",
    "            position=\"sample\",\n",
    "            description=\"模型的回答\"\n",
    "        )\n",
    "    ]\n",
    "}\n",
    "\n",
    "# 定义模型配置（这里使用占位符配置）\n",
    "model_config = {\n",
    "    \"model_name\": \"qwen-plus\",\n",
    "    \"stream\": False,\n",
    "    \"client_args\": {\n",
    "        \"timeout\": 60,\n",
    "    },\n",
    "}\n",
    "\n",
    "# 创建基于LLM的评估器\n",
    "quality_grader = LLMGrader(\n",
    "    name=\"quality_grader\",\n",
    "    mode=GraderMode.POINTWISE,\n",
    "    description=\"基于LLM的回答质量评估器\",\n",
    "    template=Template(**quality_template),\n",
    "    model=model_config,\n",
    "    rubrics=\"\"\n",
    ")\n",
    "\n",
    "print(\"基于LLM的评估器创建完成！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 准备评测数据\n",
    "\n",
    "现在我们需要准备一些测试数据。在 RM-Gallery 中，数据以 DataSample 对象的形式组织："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "准备了 2 个测试样本\n",
      "第一个样本包含 3 个回答\n",
      "第二个样本包含 3 个回答\n"
     ]
    }
   ],
   "source": [
    "# 定义数据样本的 schema\n",
    "data_sample_schema = {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"data\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"query\": {\"type\": \"string\"},\n",
    "            },\n",
    "            \"required\": [\"query\"],\n",
    "        },\n",
    "        \"samples\": {\n",
    "            \"type\": \"array\",\n",
    "            \"items\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\"answer\": {\"type\": \"string\"}},\n",
    "                \"required\": [\"answer\"],\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "    \"required\": [\"data\", \"samples\"],\n",
    "}\n",
    "\n",
    "# 创建测试数据\n",
    "raw_data_samples = [\n",
    "    {\n",
    "        \"data\": {\n",
    "            \"query\": \"什么是人工智能？\",\n",
    "        },\n",
    "        \"samples\": [\n",
    "            {\"answer\": \"人工智能是计算机科学的一个分支，它企图了解智能的实质，并生产出一种新的能以人类智能相似的方式做出反应的智能机器。\"},\n",
    "            {\"answer\": \"AI是计算机科学的一个领域。\"},\n",
    "            {\"answer\": \"人工智能就是让机器像人一样思考和学习的技术。它包括机器学习、深度学习、自然语言处理等技术。\"}\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"data\": {\n",
    "            \"query\": \"Python语言的特点有哪些？\",\n",
    "        },\n",
    "        \"samples\": [\n",
    "            {\"answer\": \"Python是一种解释型、面向对象、动态数据类型的高级程序设计语言。Python具有简洁、易读的语法，使得代码更容易维护和理解。\"},\n",
    "            {\"answer\": \"Python简单易学，有丰富的库支持。\"},\n",
    "            {\"answer\": \"\"}  # 空回答，用于测试边界情况\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "# 验证并转换为 DataSample 对象\n",
    "data_samples = validate_data_samples(raw_data_samples, data_sample_schema)\n",
    "\n",
    "print(f\"准备了 {len(data_samples)} 个测试样本\")\n",
    "print(f\"第一个样本包含 {len(data_samples[0].samples)} 个回答\")\n",
    "print(f\"第二个样本包含 {len(data_samples[1].samples)} 个回答\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 配置和使用 EvaluationRunner\n",
    "\n",
    "现在我们已经准备好了评估器和测试数据，可以配置 EvaluationRunner 来运行评估任务了。\n",
    "\n",
    "### 3.1 配置评估器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "评估器配置完成！\n"
     ]
    }
   ],
   "source": [
    "# 配置评估器\n",
    "grader_configs = {\n",
    "    \"length_evaluator\": {\n",
    "        \"grader\": length_grader(),\n",
    "    },\n",
    "    \"keyword_evaluator\": {\n",
    "        \"grader\": keyword_grader(),\n",
    "    },\n",
    "    \"quality_evaluator\": {\n",
    "        \"grader\": quality_grader,\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"评估器配置完成！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 创建和运行 EvaluationRunner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EvaluationRunner 创建完成！\n",
      "配置了 3 个评估器\n"
     ]
    }
   ],
   "source": [
    "# 创建 EvaluationRunner 实例\n",
    "# 设置最大并发数为2，避免过多的并发请求\n",
    "runner = EvaluationRunner(grader_configs=grader_configs, max_concurrent=2)\n",
    "\n",
    "print(\"EvaluationRunner 创建完成！\")\n",
    "print(f\"配置了 {len(runner.grader_configs)} 个评估器\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 运行评估任务"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始运行评估任务...\n",
      "注意：如果要实际运行基于LLM的评估器，需要配置有效的API密钥\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-11-11 20:41:48.319\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrm_gallery.core.runner.evaluation\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m90\u001b[0m - \u001b[1mResults: [{'length_evaluator': [GraderScore(reason='回答长度为 56 个字符', metadata={}, score=0.56), GraderScore(reason='回答长度为 14 个字符', metadata={}, score=0.14), GraderScore(reason='回答长度为 45 个字符', metadata={}, score=0.45)], 'keyword_evaluator': [GraderScore(reason='查询中的 1 个关键词中有 0 个在回答中找到', metadata={}, score=0.0), GraderScore(reason='查询中的 1 个关键词中有 0 个在回答中找到', metadata={}, score=0.0), GraderScore(reason='查询中的 1 个关键词中有 0 个在回答中找到', metadata={}, score=0.0)], 'quality_evaluator': [GraderScore(reason='回答准确地定义了人工智能，指出它是计算机科学的一个分支，目标是理解智能本质并制造能模拟人类智能行为的机器。内容相关、表述清晰且信息完整，但可以进一步补充一些典型应用或技术（如机器学习、自然语言处理）以提升完整性。', metadata={}, score=0.9), GraderScore(reason='回答部分相关，指出人工智能（AI）是计算机科学的一个领域，这是正确的。但回答过于简略，缺乏对人工智能具体含义、目标或应用的解释，未能完整解答‘什么是人工智能’这一问题。一个更完整的回答应包括让机器模拟人类智能行为（如学习、推理、识别等）的内容。因此，相关性尚可，准确性基本成立，但完整性不足。', metadata={}, score=0.4), GraderScore(reason='回答准确且相关，简明扼要地解释了人工智能的核心概念——让机器像人一样思考和学习，并列举了关键技术如机器学习、深度学习和自然语言处理，体现了完整性。但可以进一步补充人工智能的应用领域或目标（如推理、识别、决策等），以提升回答的全面性，因此未达到满分。', metadata={}, score=0.85)]}, {'length_evaluator': [GraderScore(reason='回答长度为 64 个字符', metadata={}, score=0.64), GraderScore(reason='回答长度为 19 个字符', metadata={}, score=0.19), GraderScore(reason='回答长度为 0 个字符', metadata={}, score=0.0)], 'keyword_evaluator': [GraderScore(reason='查询中的 1 个关键词中有 0 个在回答中找到', metadata={}, score=0.0), GraderScore(reason='查询中的 1 个关键词中有 0 个在回答中找到', metadata={}, score=0.0), GraderScore(reason='查询中的 1 个关键词中有 0 个在回答中找到', metadata={}, score=0.0)], 'quality_evaluator': [GraderScore(reason='回答准确指出了Python是解释型、面向对象、动态数据类型的高级语言，并强调了其简洁、易读的语法特点，这是Python的核心优势之一。内容相关且正确，但略显简略，未涵盖其他重要特点如丰富的标准库、跨平台性、强大的社区支持或广泛的应用领域（如数据科学、人工智能等），因此在完整性上略有欠缺。', metadata={}, score=0.85), GraderScore(reason='回答提到了Python的两个重要特点：简单易学和丰富的库支持，这是相关且准确的。但不够完整，Python的特点还包括可读性强、跨平台、动态类型、面向对象、社区支持强大等，回答缺少这些关键信息，因此完整性不足。', metadata={}, score=0.6), GraderScore(reason='提供的回答为空，没有给出任何关于Python语言特点的信息，因此无法满足相关性、准确性或完整性的基本要求。', metadata={}, score=0.0)]}]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "评估完成！\n",
      "结果: {'results': [{'length_evaluator': [GraderScore(reason='回答长度为 56 个字符', metadata={}, score=0.56), GraderScore(reason='回答长度为 14 个字符', metadata={}, score=0.14), GraderScore(reason='回答长度为 45 个字符', metadata={}, score=0.45)], 'keyword_evaluator': [GraderScore(reason='查询中的 1 个关键词中有 0 个在回答中找到', metadata={}, score=0.0), GraderScore(reason='查询中的 1 个关键词中有 0 个在回答中找到', metadata={}, score=0.0), GraderScore(reason='查询中的 1 个关键词中有 0 个在回答中找到', metadata={}, score=0.0)], 'quality_evaluator': [GraderScore(reason='回答准确地定义了人工智能，指出它是计算机科学的一个分支，目标是理解智能本质并制造能模拟人类智能行为的机器。内容相关、表述清晰且信息完整，但可以进一步补充一些典型应用或技术（如机器学习、自然语言处理）以提升完整性。', metadata={}, score=0.9), GraderScore(reason='回答部分相关，指出人工智能（AI）是计算机科学的一个领域，这是正确的。但回答过于简略，缺乏对人工智能具体含义、目标或应用的解释，未能完整解答‘什么是人工智能’这一问题。一个更完整的回答应包括让机器模拟人类智能行为（如学习、推理、识别等）的内容。因此，相关性尚可，准确性基本成立，但完整性不足。', metadata={}, score=0.4), GraderScore(reason='回答准确且相关，简明扼要地解释了人工智能的核心概念——让机器像人一样思考和学习，并列举了关键技术如机器学习、深度学习和自然语言处理，体现了完整性。但可以进一步补充人工智能的应用领域或目标（如推理、识别、决策等），以提升回答的全面性，因此未达到满分。', metadata={}, score=0.85)]}, {'length_evaluator': [GraderScore(reason='回答长度为 64 个字符', metadata={}, score=0.64), GraderScore(reason='回答长度为 19 个字符', metadata={}, score=0.19), GraderScore(reason='回答长度为 0 个字符', metadata={}, score=0.0)], 'keyword_evaluator': [GraderScore(reason='查询中的 1 个关键词中有 0 个在回答中找到', metadata={}, score=0.0), GraderScore(reason='查询中的 1 个关键词中有 0 个在回答中找到', metadata={}, score=0.0), GraderScore(reason='查询中的 1 个关键词中有 0 个在回答中找到', metadata={}, score=0.0)], 'quality_evaluator': [GraderScore(reason='回答准确指出了Python是解释型、面向对象、动态数据类型的高级语言，并强调了其简洁、易读的语法特点，这是Python的核心优势之一。内容相关且正确，但略显简略，未涵盖其他重要特点如丰富的标准库、跨平台性、强大的社区支持或广泛的应用领域（如数据科学、人工智能等），因此在完整性上略有欠缺。', metadata={}, score=0.85), GraderScore(reason='回答提到了Python的两个重要特点：简单易学和丰富的库支持，这是相关且准确的。但不够完整，Python的特点还包括可读性强、跨平台、动态类型、面向对象、社区支持强大等，回答缺少这些关键信息，因此完整性不足。', metadata={}, score=0.6), GraderScore(reason='提供的回答为空，没有给出任何关于Python语言特点的信息，因此无法满足相关性、准确性或完整性的基本要求。', metadata={}, score=0.0)]}]}\n",
      "\n",
      "评估任务结构演示完成！\n"
     ]
    }
   ],
   "source": [
    "# 运行评估任务\n",
    "# 注意：由于我们使用了基于LLM的评估器，如果要实际运行此代码，需要配置有效的API密钥\n",
    "# 在这个示例中，我们只展示代码结构\n",
    "\n",
    "print(\"开始运行评估任务...\")\n",
    "print(\"注意：如果要实际运行基于LLM的评估器，需要配置有效的API密钥\")\n",
    "\n",
    "# 如果你有有效的API密钥，可以取消注释下面的代码来实际运行评估\n",
    "results = await runner(data_samples=data_samples)\n",
    "print(\"评估完成！\")\n",
    "print(f\"结果: {results}\")\n",
    "\n",
    "print(\"\\n评估任务结构演示完成！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 结果分析\n",
    "\n",
    "评估完成后，你可以对结果进行分析。以下是如何处理评估结果的示例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "评估结果分析:\n",
      "\n",
      "样本 1:\n",
      "  length_evaluator 评估器:\n",
      "    回答 1: 分数=0.56, 理由='回答长度为 56 个字符'\n",
      "    回答 2: 分数=0.14, 理由='回答长度为 14 个字符'\n",
      "    回答 3: 分数=0.45, 理由='回答长度为 45 个字符'\n",
      "  keyword_evaluator 评估器:\n",
      "    回答 1: 分数=0.00, 理由='查询中的 1 个关键词中有 0 个在回答中找到'\n",
      "    回答 2: 分数=0.00, 理由='查询中的 1 个关键词中有 0 个在回答中找到'\n",
      "    回答 3: 分数=0.00, 理由='查询中的 1 个关键词中有 0 个在回答中找到'\n",
      "  quality_evaluator 评估器:\n",
      "    回答 1: 分数=0.90, 理由='回答准确地定义了人工智能，指出它是计算机科学的一个分支，目标是理解智能本质并制造能模拟人类智能行为的机器。内容相关、表述清晰且信息完整，但可以进一步补充一些典型应用或技术（如机器学习、自然语言处理）以提升完整性。'\n",
      "    回答 2: 分数=0.40, 理由='回答部分相关，指出人工智能（AI）是计算机科学的一个领域，这是正确的。但回答过于简略，缺乏对人工智能具体含义、目标或应用的解释，未能完整解答‘什么是人工智能’这一问题。一个更完整的回答应包括让机器模拟人类智能行为（如学习、推理、识别等）的内容。因此，相关性尚可，准确性基本成立，但完整性不足。'\n",
      "    回答 3: 分数=0.85, 理由='回答准确且相关，简明扼要地解释了人工智能的核心概念——让机器像人一样思考和学习，并列举了关键技术如机器学习、深度学习和自然语言处理，体现了完整性。但可以进一步补充人工智能的应用领域或目标（如推理、识别、决策等），以提升回答的全面性，因此未达到满分。'\n",
      "\n",
      "样本 2:\n",
      "  length_evaluator 评估器:\n",
      "    回答 1: 分数=0.64, 理由='回答长度为 64 个字符'\n",
      "    回答 2: 分数=0.19, 理由='回答长度为 19 个字符'\n",
      "    回答 3: 分数=0.00, 理由='回答长度为 0 个字符'\n",
      "  keyword_evaluator 评估器:\n",
      "    回答 1: 分数=0.00, 理由='查询中的 1 个关键词中有 0 个在回答中找到'\n",
      "    回答 2: 分数=0.00, 理由='查询中的 1 个关键词中有 0 个在回答中找到'\n",
      "    回答 3: 分数=0.00, 理由='查询中的 1 个关键词中有 0 个在回答中找到'\n",
      "  quality_evaluator 评估器:\n",
      "    回答 1: 分数=0.85, 理由='回答准确指出了Python是解释型、面向对象、动态数据类型的高级语言，并强调了其简洁、易读的语法特点，这是Python的核心优势之一。内容相关且正确，但略显简略，未涵盖其他重要特点如丰富的标准库、跨平台性、强大的社区支持或广泛的应用领域（如数据科学、人工智能等），因此在完整性上略有欠缺。'\n",
      "    回答 2: 分数=0.60, 理由='回答提到了Python的两个重要特点：简单易学和丰富的库支持，这是相关且准确的。但不够完整，Python的特点还包括可读性强、跨平台、动态类型、面向对象、社区支持强大等，回答缺少这些关键信息，因此完整性不足。'\n",
      "    回答 3: 分数=0.00, 理由='提供的回答为空，没有给出任何关于Python语言特点的信息，因此无法满足相关性、准确性或完整性的基本要求。'\n"
     ]
    }
   ],
   "source": [
    "# 示例：结果处理代码\n",
    "def analyze_results(results):\n",
    "    \"\"\"分析评估结果的示例函数\"\"\"\n",
    "    print(\"评估结果分析:\")\n",
    "    for i, sample_result in enumerate(results[\"results\"]):\n",
    "        print(f\"\\n样本 {i+1}:\")\n",
    "        for grader_name, scores in sample_result.items():\n",
    "            print(f\"  {grader_name} 评估器:\")\n",
    "            for j, score in enumerate(scores):\n",
    "                print(f\"    回答 {j+1}: 分数={score.score:.2f}, 理由='{score.reason}'\")\n",
    "\n",
    "analyze_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 总结\n",
    "\n",
    "本教程展示了如何使用 RM-Gallery 的 EvaluationRunner 进行模型评估：\n",
    "\n",
    "1. **创建评估器**：可以创建基于函数的评估器或基于LLM的评估器\n",
    "2. **准备数据**：使用 DataSample 对象组织测试数据\n",
    "3. **配置 Runner**：通过 grader_configs 配置多个评估器\n",
    "4. **运行评估**：使用 runner(data_samples) 异步运行评估任务\n",
    "5. **分析结果**：处理和分析评估结果\n",
    "\n",
    "EvaluationRunner 的主要优势：\n",
    "- 支持并发评估，提高评估效率\n",
    "- 可以同时运行多个不同的评估器\n",
    "- 提供统一的接口来处理各种类型的评估任务\n",
    "- 易于扩展和自定义评估逻辑\n",
    "\n",
    "在实际使用中，请确保：\n",
    "- 正确配置API密钥以使用基于LLM的评估器\n",
    "- 根据需要调整并发数以平衡速度和资源使用\n",
    "- 根据具体任务选择或创建合适的评估器"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rm_gallery_310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
